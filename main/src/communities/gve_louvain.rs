use crate::generic_edge::*;
use crate::generic_memory_map::*;
use crate::shared_slice::*;
use crate::utils::*;

use atomic_float::AtomicF64;
use crossbeam::thread;
use num_cpus::get_physical;
use std::sync::Arc;
use std::sync::Barrier;
use std::{
    collections::HashMap,
    sync::atomic::{AtomicBool, AtomicUsize, Ordering},
};

type ProceduralMemoryGVELouvain = (
    // K' --- renamed to k
    AbstractedProceduralMemoryMut<AtomicUsize>,
    // Œ£' --- renamed to sigma
    AbstractedProceduralMemoryMut<AtomicUsize>,
    // first CSR
    // G'.index --- renamed to gdi
    AbstractedProceduralMemoryMut<usize>,
    // G'.edges --- renamed to gde
    AbstractedProceduralMemoryMut<(usize, usize)>,
    // second CSR
    // G''.index --- renamed to gddi
    AbstractedProceduralMemoryMut<usize>,
    // G''.edges --- renamed to gdde
    AbstractedProceduralMemoryMut<(usize, usize)>,
    // processed
    AbstractedProceduralMemoryMut<AtomicBool>,
    // C' --- renamed to coms (C is stored as a member of the struct)
    AbstractedProceduralMemoryMut<usize>,
    // Helper array for C' renumbering, dendrogram lookup & CSR aggregation
    AbstractedProceduralMemoryMut<usize>,
);

#[allow(dead_code)]
#[derive(Debug)]
pub struct AlgoGVELouvain<'a, EdgeType: GenericEdgeType, Edge: GenericEdge<EdgeType>> {
    /// the graph for which the partition is computed
    graph: &'a GraphMemoryMap<EdgeType, Edge>,
    /// memmapped array containing each node's community
    community: AbstractedProceduralMemoryMut<usize>,
    /// cardinality of distinct communities in the final partition
    community_count: usize,
    /// partition modularity
    modularity: f64,
}
#[allow(dead_code)]
impl<'a, EdgeType: GenericEdgeType, Edge: GenericEdge<EdgeType>>
    AlgoGVELouvain<'a, EdgeType, Edge>
{
    /// constants set according to the optimizeds parameters described in "GVE-Louvain: Fast Louvain Algorithm for
    /// Community Detection in Shared Memory Setting".
    /// Described in 4.1.2 Limiting the number of iterations per pass
    const MAX_ITERATIONS: usize = 20;
    /// Described in 4.1.3 Adjusting tolerance drop rate (threshold scaling)
    const TOLERANCE_DROP: f64 = 10.;
    /// Described in 4.1.4 Adjusting initial tolerance
    const INITIAL_TOLERANCE: f64 = 0.01;
    /// Described in 4.1.4 Adjusting aggregation tolerance
    const AGGREGATION_TOLERANCE: f64 = 0.8;
    /// Maximum number of passes to be performed (a pass is an iteration of the Louvain() function described in "GVE-Louvain: Fast Louvain Algorithm for
    /// Community Detection in Shared Memory Setting" p. 5).
    const MAX_PASSES: usize = 30;

    /// Performs the Louvain() function as described in "GVE-Louvain: Fast Louvain Algorithm for
    /// Community Detection in Shared Memory Setting" p. 5.
    ///
    /// The resulting graph partition and its corresponding modularity are stored in memory (in
    /// the partition's case in a memmapped file).
    ///
    /// * Note: isolated nodes remain in their own isolated community, in the final partition.
    ///
    /// # Arguments
    ///
    /// * `graph`: `&GraphMemoryMap<EdgeType, Edge>` --- the graph for which the louvain partition is to be computed.
    pub fn new(
        graph: &'a GraphMemoryMap<EdgeType, Edge>,
    ) -> Result<Self, Box<dyn std::error::Error>> {
        let output_filename =
            cache_file_name(graph.cache_fst_filename(), FileType::GVELouvain, None)?;
        let community =
            SharedSliceMut::<usize>::abst_mem_mut(output_filename.clone(), graph.width(), true)?;
        let mut gve_louvain = Self {
            graph,
            community,
            community_count: 0,
            modularity: 0.,
        };
        gve_louvain.compute(10)?;
        Ok(gve_louvain)
    }

    /// Returns the number of communities in the Louvain partition.
    pub fn community_count(&self) -> usize {
        self.community_count
    }

    /// Returns the modularity of the Louvain partition.
    pub fn partition_modularity(&self) -> f64 {
        self.modularity
    }

    fn init_procedural_memory_gve_louvain(
        &self,
        mmap: u8,
    ) -> Result<ProceduralMemoryGVELouvain, Box<dyn std::error::Error>> {
        let edge_count = self.graph.width();
        let node_count = self.graph.size() - 1;

        let template_fn = self.graph.cache_index_filename();
        let k_fn = cache_file_name(template_fn.clone(), FileType::GVELouvain, Some(0))?;
        let s_fn = cache_file_name(template_fn.clone(), FileType::GVELouvain, Some(1))?;
        let gdi_fn = cache_file_name(template_fn.clone(), FileType::GVELouvain, Some(2))?;
        let gde_fn = cache_file_name(template_fn.clone(), FileType::GVELouvain, Some(3))?;
        let gddi_fn = cache_file_name(template_fn.clone(), FileType::GVELouvain, Some(4))?;
        let gdde_fn = cache_file_name(template_fn.clone(), FileType::GVELouvain, Some(5))?;
        let a_fn = cache_file_name(template_fn.clone(), FileType::GVELouvain, Some(6))?;
        let c_fn = cache_file_name(template_fn.clone(), FileType::GVELouvain, Some(7))?;
        let nc_fn = cache_file_name(template_fn.clone(), FileType::GVELouvain, Some(8))?;

        let k = SharedSliceMut::<AtomicUsize>::abst_mem_mut(k_fn, node_count, mmap > 0)?;
        let sigma = SharedSliceMut::<AtomicUsize>::abst_mem_mut(s_fn, node_count, mmap > 0)?;
        let gdi = SharedSliceMut::<usize>::abst_mem_mut(gdi_fn, 2 * node_count, mmap > 0)?;
        let gde = SharedSliceMut::<(usize, usize)>::abst_mem_mut(gde_fn, edge_count, mmap > 0)?;
        let gddi = SharedSliceMut::<usize>::abst_mem_mut(gddi_fn, 2 * node_count, mmap > 0)?;
        let gdde = SharedSliceMut::<(usize, usize)>::abst_mem_mut(gdde_fn, edge_count, mmap > 0)?;
        let processed = SharedSliceMut::<AtomicBool>::abst_mem_mut(a_fn, node_count, mmap > 0)?;
        let coms = SharedSliceMut::<usize>::abst_mem_mut(c_fn, node_count, mmap > 0)?;
        // has to be size |E| as it is used to store the 'holey' CSR
        let helper = SharedSliceMut::<usize>::abst_mem_mut(nc_fn, edge_count, true)?;

        Ok((k, sigma, gdi, gde, gddi, gdde, processed, coms, helper))
    }

    /// Computes the differencial modularity upon moving a node `u`, from a community `d` to a
    /// community `c`:
    ///
    /// ùõøùëÑ = (Ku->c - Ku->d) / m - (Ku * (Œ£c - Œ£d) / (2m^2)).
    ///
    /// # Arguments
    ///
    /// * `k_u_c`: f64 --- The weight of edges from node `u` to nodes in community `c`.
    /// * `k_u_d`: f64 --- The weight of edges from node `u` to nodes in community `d`.
    /// * `k_u`: f64 --- Total weight of edges from node `u`.
    /// * `sig_c`: f64 --- Total weight of edges in community `c`.
    /// * `sig_d`: f64 --- Total weight of edges in community `d`.
    /// * `m`: f64 --- Total weight of edges in the graph (or half of it if graphis directed).
    ///
    /// # Returns
    ///
    /// Difference in modularity upon moving node `u` from community `d` to community `c`.
    #[inline(always)]
    fn delta_q(k_u_c: f64, k_u_d: f64, k_u: f64, sig_c: f64, sig_d: f64, m: f64) -> f64 {
        assert!(k_u_c >= 0. && k_u_d >= 0. && k_u >= 0. && sig_c >= 0. && sig_d >= 0. && m >= 0.);
        assert!(
            k_u_c.is_finite()
                && k_u_d.is_finite()
                && k_u.is_finite()
                && sig_c.is_finite()
                && sig_d.is_finite()
                && m.is_normal()
        );
        let dq = (k_u_c - k_u_d) / m - k_u * (k_u + sig_c - sig_d) / (2. * m * m);
        if dq > 1. {
            println!(
                "Deviant dq: ({k_u_c} - {k_u_d}) / {m} - {k_u} * ({k_u} + {sig_c} - {sig_d}) / (2. *{m} * {m}) = {dq}"
            );
        }
        dq
    }

    /// Determines the total weight of edges from a given node `u` to every community `c` it is linked to.
    ///
    /// # Arguments
    ///
    /// * `nmap`: &mut HashMap<usize, usize> (updated) --- Hashmap for entries `(c, k_u_c)`, where `c` is a community and `k_u_c` is the total weight of edges from node u into nodes in `c`.
    /// * `edges`: SharedSliceMut<(usize, usize)> --- Edges vector, should consist of entries `(dest_node: usize, edge_weight: usize)`.
    /// * `index`: SharedSliceMut<usize> --- Index vector.
    /// * `coms`: SharedSliceMut<usize> --- Communities vector.
    /// * `u`: usize --- Index of node `u`.
    /// * `self_allowed`: bool --- Flag signaling whether self-loops should be accepted.
    #[inline(always)]
    fn scan_communities(
        comm_count: usize,
        nmap: &mut HashMap<usize, usize>,
        edges: SharedSliceMut<(usize, usize)>,
        index: SharedSliceMut<usize>,
        coms: SharedSliceMut<usize>,
        u: usize,
        self_allowed: bool,
    ) {
        for e_idx in *index.get(2 * u)..*index.get(2 * u + 1) {
            let e = edges.get(e_idx);
            if !self_allowed && e.0 == u {
                continue;
            }
            if *coms.get(e.0) > comm_count {
                println!(
                    "WARNING!! Impossible neighbour commnity for node {u} (neighbour {}): {{{} > {comm_count}}} but c: {}",
                    e.0,
                    *coms.get(e.0),
                    *coms.get(*coms.get(e.0))
                );
                println!(
                    "{}..{} at {e_idx}\n",
                    *index.get(2 * u),
                    *index.get(2 * u + 1)
                );
            }
            // ùêªùë°[ùê∂'[v]] ‚Üê ùêªùë°[ùê∂'[v]] + w
            *nmap.entry(*coms.get(e.0)).or_insert(0) += e.1;
        }
    }

    /// Determines the best community a node `u` may be switched into from its neighbours community set.
    ///
    /// # Arguments
    ///
    /// * `nmap`: &mut HashMap<usize, usize> --- Hashmap of entries `(c, k_u_c)`, where `c` is a community and `k_u_c` is the total weight of edges from node `u` into nodes in `c`.
    /// * `sigma`: SharedSliceMut<AtomicUsize> --- Total community weights vector.
    /// * `c_u`: usize --- Community of node `u`.
    /// * `k:u`: f64 --- Total weight of node `u` <<f64>>.
    /// * `m`: f64 --- Total weight of edges in the graph (or half of it if graph is directed) <<f64>>.
    ///
    /// # Returns
    ///
    /// `(c*: usize, ùõøùëÑ*: f64)`, where:
    /// * `c*` is the best community node u may be switched into
    /// * `ùõøùëÑ*` is the delta modularity arising thereof.
    #[inline(always)]
    fn choose_community(
        nmap: &mut HashMap<usize, usize>,
        sigma: SharedSliceMut<AtomicUsize>,
        c_u: usize,
        k_u: f64,
        m: f64,
    ) -> (usize, f64) {
        // houses c*
        let mut best_gain = 0.;
        // houses ùõøùëÑ*
        let mut best_comm = c_u;

        // Ku->d
        let k_u_d = nmap.get(&c_u).cloned().unwrap_or(0) as f64;
        // Œ£d
        let sig_d = sigma.get(c_u).load(Ordering::Relaxed) as f64;

        for (&c_v, &edge_weight) in nmap.iter() {
            if c_v == c_u {
                continue;
            }

            // Ku->c
            let k_u_c = edge_weight as f64;
            // Œ£c
            let sig_c = sigma.get(c_v).load(Ordering::Relaxed) as f64;

            // ùõøùëÑ = (Ku->c - Ku->d) / m - (Ku * (Œ£c - Œ£d) / (2m^2))
            let dq = Self::delta_q(k_u_c, k_u_d, k_u, sig_c, sig_d, m);
            if dq > best_gain {
                // ùõøùëÑ‚àó = ùõøùëÑ
                best_gain = dq;
                // *c = C'[v]
                best_comm = c_v;
            }
        }
        // (c*, ùõøùëÑ*)
        (best_comm, best_gain)
    }

    /// Performs the LouvainMove() function as described in "GVE-Louvain: Fast Louvain Algorithm for
    /// Community Detection in Shared Memory Setting" p. 6.
    #[inline(always)]
    #[allow(clippy::too_many_arguments)]
    fn louvain_move(
        l_pass: usize,
        comm_count: usize,
        k: SharedSliceMut<AtomicUsize>,
        sigma: SharedSliceMut<AtomicUsize>,
        processed: SharedSliceMut<AtomicBool>,
        idx: SharedSliceMut<usize>,
        mut next_index: SharedSliceMut<usize>,
        e: SharedSliceMut<(usize, usize)>,
        mut c: SharedSliceMut<usize>,
        mut helper: SharedSliceMut<usize>,
        m: f64,
        tolerance: f64,
        threads: usize,
    ) -> Result<usize, Box<dyn std::error::Error>> {
        // recalculate node load per thread
        let node_load = comm_count.div_ceil(threads);

        let mut l_iter = 0;
        let synchronize = Arc::new(Barrier::new(threads));
        for i in 0..Self::MAX_ITERATIONS {
            let global_delta_q = Arc::new(AtomicF64::new(0.0));

            // reset iteration metrics
            thread::scope(|scope| -> Result<(), Box<dyn std::error::Error>> {
                // use dynamic style scheduling by splitting range [0, cur_node_count) for threads
                let mut thread_res = Vec::with_capacity(threads);
                for tid in 0..threads {
                    let k = k.clone();
                    let sigma = sigma.clone();
                    let processed = processed.clone();

                    let global_delta_q = global_delta_q.clone();
                    let synchronize = synchronize.clone();

                    let begin = std::cmp::min(tid * node_load, comm_count);
                    let end = std::cmp::min(begin + node_load, comm_count);

                    // each thread gets its own local HashMap for accumulating neighbor community weights
                    thread_res.push(scope.spawn(
                        move |_| -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
                            // if pass > 0, we need to initialize communities for the new graph (each super-vertex in its own community)
                            if l_pass != 0 && i == 0 {
                                for u in begin..end {
                                    // unprune all nodes
                                    *helper.get_mut(u) = usize::MAX;
                                    *next_index.get_mut(u * 2) = 0;
                                    // reassign node community to itself
                                    *c.get_mut(u) = u;
                                }
                            }

                            // wait for all threads to finish current pass initialization
                            synchronize.wait();

                            let mut local_delta_q_sum = 0.0;
                            let mut nmap: HashMap<usize, usize> = HashMap::new();
                            for u in begin..end {
                                // skip this vertex if it was marked as pruned (no need to process)
                                if processed.get(u).swap(true, Ordering::Relaxed) {
                                    continue;
                                }
                                let c_u = *c.get(u);
                                // get weight of u k[u]
                                let k_u = k.get(u).load(Ordering::Relaxed);
                                // gather neighbour communities and edge weight sums using the adjacency list of u

                                nmap.clear();
                                let nm = &mut nmap;
                                Self::scan_communities(comm_count, nm, e, idx, c, u, false);

                                let s = sigma.clone();
                                let (b_c, b_g) = Self::choose_community(nm, s, c_u, k_u as f64, m);

                                // move u to the best community if it yields positive gain
                                if b_c != c_u && b_g > 1e-12 {
                                    // atomic updates to community totals --- if node has already
                                    // been moved don't do anything
                                    if sigma
                                        .get(c_u)
                                        .fetch_update(Ordering::Relaxed, Ordering::Relaxed, |s| {
                                            if s < k_u {
                                                None
                                            } else {
                                                Some(s - k_u)
                                            }
                                        })
                                        .is_ok()
                                    {
                                        sigma.get(b_c).fetch_add(k_u, Ordering::Relaxed);
                                        // update community assignment of u
                                        *c.get_mut(u) = b_c;
                                        local_delta_q_sum += b_g;
                                        // mark all neighbors of u to be processed in the next iteration
                                        for idx in *idx.get(u * 2)..*idx.get(u * 2 + 1) {
                                            processed
                                                .get(e.get(idx).0)
                                                .store(false, Ordering::Relaxed);
                                        }
                                    } else {
                                        return Err(
                                            format!("error louvain_move(): {} {u} can't be removed from its community {c_u} as node weight is {k_u} but community total weight is only {}", 
                                                    if l_pass == 0 {"node"} else {"super-node"}, sigma.get(c_u).load(Ordering::Relaxed)
                                                    ).into()
                                            );
                                    }
                                }
                            }
                            // atomically accumulate thread-local result to global
                            global_delta_q.fetch_add(local_delta_q_sum, Ordering::Relaxed);
                            Ok(())
                        },
                    ));
                }
                for (i, r) in thread_res.into_iter().enumerate() {
                    let t_res = r.join();
                    if t_res.is_err() {
                        return Err(
                            format!("error joining {i}, couldn't get ùõøùëÑ: {:?}", t_res).into()
                        );
                    }
                }
                Ok(())
            })
            .unwrap()?;

            // check convergence criteria for the local-moving phase
            if global_delta_q.load(Ordering::Relaxed).abs() < tolerance {
                break;
            }
            l_iter += 1;
        }
        Ok(l_iter)
    }

    /// Performs the LouvainAggregate() function as described in "GVE-Louvain: Fast Louvain Algorithm for
    /// Community Detection in Shared Memory Setting" p. 6.
    #[inline(always)]
    #[allow(clippy::too_many_arguments)]
    fn louvain_aggregate(
        l_pass: usize,
        comm_count: usize,
        new_comm_count: usize,
        k: SharedSliceMut<AtomicUsize>,
        sigma: SharedSliceMut<AtomicUsize>,
        processed: SharedSliceMut<AtomicBool>,
        index: SharedSliceMut<usize>,
        mut next_index: SharedSliceMut<usize>,
        edges: SharedSliceMut<(usize, usize)>,
        mut next_edges: SharedSliceMut<(usize, usize)>,
        coms: SharedSliceMut<usize>,
        mut helper: SharedSliceMut<usize>,
        threads: usize,
    ) -> Result<(), Box<dyn std::error::Error>> {
        // recalculate comm load per thread
        let prev_comm_load = comm_count.div_ceil(threads);
        let new_comm_load = new_comm_count.div_ceil(threads);

        let synchronize = Arc::new(Barrier::new(threads));

        // exclusive scan
        let mut sum = 0;
        for u in 0..new_comm_count {
            let degree = *next_index.get(u * 2);
            *next_index.get_mut(u * 2) = sum;
            k.get(u).store(sum, Ordering::SeqCst);
            sum += degree;
        }

        thread::scope(|scope| -> Result<(), Box<dyn std::error::Error>> {
            // use dynamic style scheduling by splitting range [0, cur_node_count) for threads
            let mut thread_res = Vec::with_capacity(threads);
            for tid in 0..threads {
                // local accumulator variables
                let mut total = 0;
                let mut total_weight = 0;

                // hashmap to store edges for each community
                let mut nm: HashMap<usize, usize> = HashMap::new();

                let k = k.clone();
                let sigma = sigma.clone();
                let processed = processed.clone();

                let synchronize = synchronize.clone();

                let prev_begin = std::cmp::min(tid * prev_comm_load, comm_count);
                let prev_end = std::cmp::min(prev_begin + prev_comm_load, comm_count);
                let new_begin = std::cmp::min(tid * new_comm_load, new_comm_count);
                let new_end = std::cmp::min(new_begin + new_comm_load, new_comm_count);

                thread_res.push(scope.spawn(
                    move |_| -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
                        // add com - node edges atomically
                        for u in prev_begin..prev_end {
                            let degree = *index.get(u * 2 + 1) - *index.get(u * 2);
                            // guard against isolated nodes to not go over community index boundary
                            if degree > 0 {
                                // add edge (C'[u], u)
                                let idx = k.get(*coms.get(u)).fetch_add(1, Ordering::SeqCst);
                                *helper.get_mut(idx) = u;
                            }
                        }

                        // wait for every thread to finish adding com - node edges
                        synchronize.wait();

                        for c in new_begin..new_end {
                            *next_index.get_mut(c * 2 + 1) = k.get(c).load(Ordering::Relaxed);
                        }

                        // wait for every thread to finish com - node CSR indexing
                        synchronize.wait();

                        for c in new_begin..new_end {
                            // mark unprocessed
                            processed.get(c).store(false, Ordering::Relaxed);

                            // clear previous community's neighbour map
                            nm.clear();

                            // for every node in the community scan edges to accumulate supergraph
                            // edge weights
                            for e_idx in *next_index.get(c * 2)..k.get(c).load(Ordering::Relaxed) {
                                Self::scan_communities(comm_count, &mut nm, edges, index, coms, *helper.get(e_idx), true);
                            }

                            // for every linked community store (dest, weight) in CSR
                            for (&d, &w) in nm.iter() {
                                // troubleshoot edge
                                if d > new_comm_count {
                                    return Err(
                                        format!("error building supergraph: found edge to community {d} but max_id for communities is {comm_count} in the {l_pass}th pass"
                                                ).into()
                                        );
                                }

                                // store edge
                                *next_edges.get_mut(*next_index.get(c * 2) + total) = (d, w);

                                // increment counts
                                total += 1;
                                total_weight += w;
                            }

                            // set cmommunity index upper boundary
                            *next_index.get_mut(c * 2 + 1) = *next_index.get(c * 2) + total;

                            // check for indexing missalignment errors
                            if c > 0 && *next_index.get(c * 2) < *next_index.get(c * 2 - 1) {
                                println!("error {} {} {}", new_begin, *next_index.get(c * 2), *next_index.get(c * 2 - 1));
                                return Err(
                                    format!("error building supergraph: index missalignment, node {} ends at {} and node {c} starts at {}",
                                            c - 1, *next_index.get(c * 2 - 1), *next_index.get(c * 2)
                                            ).into()
                                    );
                            }
                            // store node weight & community weight
                            k.get(c).store(total_weight, Ordering::Relaxed);
                            sigma.get(c).store(total_weight, Ordering::Relaxed);

                            // reset for next comm
                            total = 0;
                            total_weight = 0;
                        }
                        Ok(())
                    },
                ));
            }
            for (i, r) in thread_res.into_iter().enumerate() {
                let t_res = r.join();
                if t_res.is_err() {
                    return Err(format!("error joining {i} in aggregation: {:?}", t_res).into());
                }
            }
            Ok(())
        })
        .unwrap()?;

        Ok(())
    }

    fn compute(&mut self, mmap: u8) -> Result<(), Box<dyn std::error::Error>> {
        let edge_count = self.graph.width();
        let node_count = self.graph.size() - 1;

        let threads = self.graph.thread_num().max(get_physical());
        let node_load = node_count.div_ceil(threads);

        let (k, sigma, gdi, gde, gddi, gdde, processed, coms, helper) =
            self.init_procedural_memory_gve_louvain(mmap)?;

        // C --- the community vector of the original nodes
        let mut communities = self.community.shared_slice();
        let index_ptr = SharedSlice::<usize>::new(self.graph.index_ptr(), node_count + 1);
        let graph_ptr = SharedSlice::<Edge>::new(self.graph.edges_ptr(), edge_count);

        // initialize
        thread::scope(|scope| -> Result<(), Box<dyn std::error::Error>> {
            let mut threads_res = vec![];
            for tid in 0..threads {
                let processed = processed.shared_slice();

                let mut gdi = gdi.shared_slice();
                let mut gde = gde.shared_slice();
                let mut k = k.shared_slice();
                let mut sigma = sigma.shared_slice();
                let mut coms = coms.shared_slice();
                let mut helper = helper.shared_slice();

                let begin = std::cmp::min(tid * node_load, node_count);
                let end = std::cmp::min(begin + node_load, node_count);

                threads_res.push(scope.spawn(
                    move |_| -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
                        for u in begin..end {
                            let u_start = *index_ptr.get(u);
                            let u_end = *index_ptr.get(u + 1);
                            let out_offset = u * 2;
                            let deg_u = match u_end.overflowing_sub(u_start) {
                                (r, false) => r,
                                (_, true) => {
                                    return Err(format!(
                                        "overflow calculating degree of {u}: {u_end} - {u_start}"
                                    )
                                    .into());
                                }
                            };
                            let edges = match graph_ptr.slice(u_start, u_end) {
                                Some(e) => e
                                    .iter()
                                    .map(|e| (e.dest(), 1))
                                    .collect::<Vec<(usize, usize)>>(),
                                None => {
                                    return Err(format!("error reading node {u} in init").into());
                                }
                            };

                            // mark all nodes unprocessed
                            processed.get(u).swap(false, Ordering::Relaxed);

                            // initilize helper, communities, coms and CSR vectors
                            *helper.get_mut(u) = usize::MAX;
                            *communities.get_mut(u) = u;
                            *coms.get_mut(u) = u;
                            *gdi.get_mut(out_offset) = u_start;
                            *gdi.get_mut(out_offset + 1) = u_end;

                            // copy G onto G'
                            gde.write_slice(u_start, edges.as_slice());

                            // Œ£‚Ä≤ ‚Üê ùêæ‚Ä≤ ‚Üê ùë£ùëíùëüùë°ùëíùë•_wùëíùëñùëî‚Ñéùë°ùë†(ùê∫‚Ä≤)
                            *k.get_mut(u) = AtomicUsize::new(deg_u);
                            *sigma.get_mut(u) = AtomicUsize::new(deg_u);
                        }
                        Ok(())
                    },
                ));
            }

            // check for errors
            for (tid, r) in threads_res.into_iter().enumerate() {
                let t_res = r.join();
                if t_res.is_err() {
                    return Err(format!("error (thread {tid}): {:?}", t_res).into());
                }
            }

            Ok(())
        })
        .unwrap()?;

        let mut tolerance = Self::INITIAL_TOLERANCE;

        let mut prev_comm_count = node_count;
        let mut new_comm_count: usize = 0;

        // 2m (since each undirected edge has weight 2)
        let m2 = edge_count as f64;
        let m = m2 / 2.;

        let mut coms = coms.shared_slice();
        let mut helper = helper.shared_slice();

        let mut index = gdi.shared_slice();
        let mut next_index = gddi.shared_slice();
        let mut edges = gde.shared_slice();
        let mut next_edges = gdde.shared_slice();

        let k = k.shared_slice();
        let sigma = sigma.shared_slice();
        let processed = processed.shared_slice();

        // outer loop: louvain passes
        for l_pass in 0..Self::MAX_PASSES {
            if prev_comm_count <= 1 {
                break;
            }

            // Œ£‚Ä≤ ‚Üê ùêæ‚Ä≤ ‚Üê ùë£ùëíùëüùë°ùëíùë•_wùëíùëñùëî‚Ñéùë°ùë†(ùê∫‚Ä≤) ; ùê∂‚Ä≤ ‚Üê [0..|ùëâ‚Ä≤|) (initialization inside louvain_move())
            // li <- louvain_move(G', C', K', Œ£‚Ä≤)
            // if ùëôùëñ ‚â§ 1 then break --- adjusted to ùëôùëñ ‚â§ 0 as we don't increment before loop ends
            // as is teh case with the algorithm author
            let count_iter = Self::louvain_move(
                l_pass,
                prev_comm_count,
                k.clone(),
                sigma.clone(),
                processed.clone(),
                index,
                next_index,
                edges,
                coms,
                helper,
                m,
                tolerance,
                threads,
            )?;
            if count_iter == 0 {
                break;
            }

            for u in 0..prev_comm_count {
                let degree = *index.get(u * 2 + 1) - *index.get(u * 2);
                let c = *coms.get(u);
                let new_c = *helper.get(c);
                if new_c == usize::MAX {
                    *helper.get_mut(c) = new_comm_count;
                    *coms.get_mut(u) = new_comm_count;
                    *next_index.get_mut(new_comm_count * 2) = degree;
                    new_comm_count += 1;
                } else {
                    *coms.get_mut(u) = new_c;
                    *next_index.get_mut(new_c * 2) += degree;
                }
            }

            // ùê∂ ‚Üê Lookup dendrogram using ùê∂ to ùê∂
            for orig in 0..node_count {
                let c = *communities.get(orig);
                if c < prev_comm_count {
                    *communities.get_mut(orig) = *coms.get(c);
                } else {
                    return Err(
                        format!(
                            "error performing dendrogram lookup: node {orig} is in community {c} but max_id for communities is {prev_comm_count} in the {l_pass}th pass"
                            ).into()
                        );
                }
            }

            // if |Œì|/|Œìùëúùëôùëë | > ùúè_ùëéùëîùëî then break
            if new_comm_count as f64 / prev_comm_count as f64 > Self::AGGREGATION_TOLERANCE {
                prev_comm_count = new_comm_count;
                break;
            }

            // build the aggregated graph (super-vertex graph) in the 'next_index' and 'next_edges' buffers
            Self::louvain_aggregate(
                l_pass,
                prev_comm_count,
                new_comm_count,
                k.clone(),
                sigma.clone(),
                processed.clone(),
                index,
                next_index,
                edges,
                next_edges,
                coms,
                helper,
                threads,
            )?;

            // check aggregation tolerance: if communities did not reduce sufficiently, break cycle
            if new_comm_count as f64 >= (prev_comm_count as f64 * Self::AGGREGATION_TOLERANCE) {
                prev_comm_count = new_comm_count;
                break;
            }

            // tighten tolerance for next pass (threshold scaling)
            // ùúè = ùúè / TOLERANCE_DROP
            tolerance /= Self::TOLERANCE_DROP;

            // prepare for next loop by resetting community count and switching CSRs
            std::mem::swap(&mut index, &mut next_index);
            std::mem::swap(&mut edges, &mut next_edges);

            prev_comm_count = new_comm_count;
            new_comm_count = 0;
        }

        self.community_count = prev_comm_count;

        // compute partition modularity
        if m2 == 0. {
            self.modularity = 0.0;
            return Ok(());
        }

        for u in 0..node_count {
            k.get(*communities.get(u)).store(0, Ordering::Relaxed);
            sigma.get(*communities.get(u)).store(0, Ordering::Relaxed);
        }

        // sum internal degree ---> sigma | sum total degree ---> k
        for u in 0..node_count {
            let comm_u = *communities.get(u);
            let iter = self.graph.neighbours(u)?;
            k.get(*communities.get(u))
                .fetch_add(iter.remaining_neighbours(), Ordering::Relaxed);
            for e in iter {
                let v = e.dest();
                if v >= node_count {
                    continue;
                } // safety
                if *communities.get(v) == comm_u {
                    sigma.get(comm_u).fetch_add(1, Ordering::Relaxed);
                }
            }
        }

        // modularity: sum_c( internal_dir_edges[c]/m2 - (Kc[c]/m2)^2 )
        let mut partition_modularity = 0.0f64;
        for c in 0..prev_comm_count {
            let lc_over_m2 = (sigma.get(c).load(Ordering::Relaxed) as f64) / m2;
            let kc_over_m2 = (k.get(c).load(Ordering::Relaxed) as f64) / m2;
            partition_modularity += lc_over_m2 - kc_over_m2 * kc_over_m2;
        }

        self.modularity = partition_modularity;
        self.community.flush()?;

        cleanup_cache()?;

        Ok(())
    }
}
